{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR KNN v0.4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNFTOAjFW3xd",
        "colab_type": "code",
        "outputId": "8b70a779-ff83-489f-9ab3-ae3b7b7e4bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_Gqp-rrYEtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpxXP1CAW_qE",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing Function for words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6HZ1sq1W6X3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import imutils\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def preprocess(img, imgSize, dataAugmentation=False):\n",
        "\t\"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
        "\n",
        "\t# there are damaged files in IAM dataset - just use black image instead\n",
        "\tif img is None:\n",
        "\t\timg = np.zeros([imgSize[1], imgSize[0]])\n",
        "\t\tprint('test')\n",
        "\timg = cv2.resize(img, imgSize)\n",
        "\timg = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\t# imgBlurred = cv2.GaussianBlur(img, (5,5), 0)                        # blur\n",
        "\n",
        "  # #                                                       # filter image from grayscale to black and white\n",
        "\t# imgThresh = cv2.adaptiveThreshold(img,                           # input image\n",
        "  #                                     255,                                  # make pixels that pass the threshold full white\n",
        "  #                                     cv2.ADAPTIVE_THRESH_GAUSSIAN_C,       # use gaussian rather than mean, seems to give better results\n",
        "  #                                     cv2.THRESH_BINARY_INV,                # invert so foreground will be white, background will be black\n",
        "  #                                     11,                                   # size of a pixel neighborhood used to calculate threshold value\n",
        "  #                                     2)                                    # constant subtracted from the mean or weighted mean\n",
        "\n",
        "  #   # cv2.imshow(\"imgThresh\", imgThresh)\n",
        "\tcv2_imshow(img)  \n",
        "\t# # return imgThresh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mp3Qf-AC08-K",
        "outputId": "2ffeeb32-6257-4ee6-a5ff-0b98b0d7d34a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "img = cv2.imread('/content/drive/My Drive/APM Project/Data/Characters only/Img/Sample029/img029-003.png', cv2.IMREAD_UNCHANGED) #change the path here\n",
        "imgSize = (43, 32)\n",
        "test = cv2.resize(img, imgSize, interpolation = cv2.INTER_AREA)\n",
        "# cv2_imshow(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4a0ece1bcd60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/APM Project/Data/Characters only/Img/Sample029/img029-003.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_UNCHANGED\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#change the path here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimgSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m43\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# cv2_imshow(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(3.4.3) /io/opencv/modules/imgproc/src/resize.cpp:4044: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQWJRhnvXi_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_labels = pd.read_csv('/content/drive/My Drive/APM Project/Data/Characters only/Label file.csv',header = None)\n",
        "image_labels.columns = ['Image Path','Label','Mapping']\n",
        "\n",
        "imgSize = (43, 32)\n",
        "character_data = pd.DataFrame(columns = ['Character_Image', 'Character_Label','Mapped_Label'])\n",
        "\n",
        "for i in range(len(image_labels)):\n",
        "  img = preprocess(np.uint8(cv2.imread('/content/drive/My Drive/APM Project/Data/Characters only/'+image_labels['Image Path'][i])), imgSize)\n",
        "  character_data= character_data.append({'Character_Image':img, 'Character_Label':image_labels['Label'][i], 'Mapped_Label':np.int64(image_labels['Mapping'][i])}, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-13IUJ3xcSYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "temp = np.array(character_data['Character_Image'].to_list())\n",
        "X_train, X_test, y_train, y_test = train_test_split(temp, character_data['Mapped_Label'], test_size=0.10, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVFnCmsnh1PA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we prepare train_data and test_data after resizing to 32X43.\n",
        "X_train = X_train.reshape(-1,1376).astype(np.float32)\n",
        "X_test = X_test.reshape(-1,1376).astype(np.float32) \n",
        "# Create labels for train and test data\n",
        "y_train = y_train.to_numpy(dtype='int64')[:,np.newaxis]\n",
        "y_test = y_test.to_numpy(dtype = 'int64')[:,np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TO0T23QCoLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "# Fit on training set only.\n",
        "scaler.fit(X_train)\n",
        "# Apply transform to both the training set and the test set.\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXwecnBhE-7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# Make an instance of the Model\n",
        "pca = PCA(.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_XL5nLUFBbJ",
        "colab_type": "code",
        "outputId": "3985520d-619b-4a08-c7fa-27a4b22541fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pca.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,\n",
              "    svd_solver='auto', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePxXPXGUFHMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = pca.transform(X_train)\n",
        "X_test = pca.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suCQWPvUHUVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = pd.DataFrame(data=X_train[0:,0:], index=[i for i in range(X_train.shape[0])], columns=['f'+str(i) for i in range(X_train.shape[1])]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X90aQStfHd0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = pd.DataFrame(data=X_test[0:,0:], index=[i for i in range(X_test.shape[0])], columns=['f'+str(i) for i in range(X_test.shape[1])]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPaDN4kQdu-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Initiate kNN, train the data, then test it with test data for k=1\n",
        "# knn = cv2.ml.KNearest_create()\n",
        "# knn.train(X_train, cv2.ml.ROW_SAMPLE, y_train)\n",
        "# ret, result, neighbours, dist = knn.findNearest(X_test, k=3)\n",
        "\n",
        "# # Now we check the accuracy of classification\n",
        "# # For that, compare the result with test_labels and check which are wrong\n",
        "# matches = result==y_test\n",
        "# correct = np.count_nonzero(matches)\n",
        "# accuracy = correct*100.0/result.size\n",
        "# print (accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhMGjFztBwcO",
        "colab_type": "code",
        "outputId": "72a4db69-16d0-46a5-a780-5cde8384149b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_model = KNeighborsClassifier(n_neighbors = 3)\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred = knn_model.predict(X_test)\n",
        "acc = knn_model.score(X_test, y_test)\n",
        "print('Accuracy: {:.2f}%'.format(acc*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 36.95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8bYce8ATXtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = knn_model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ_OGhY8TmsU",
        "colab_type": "code",
        "outputId": "a82e433e-1087-4d97-a894-4ca7a17f49f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch6hSvsGTq3c",
        "colab_type": "code",
        "outputId": "3ad4f78c-28d9-4d47-8bf8-7878de53f3c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "cv2_imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-310-796659f66216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2429\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many dimensions: %d > %d.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2431\u001b[0;31m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tobytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2VlEWDkKjlX",
        "colab_type": "code",
        "outputId": "e735f800-557e-4532-a2a2-da056daa9560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=1000)\n",
        "rfc.fit(X_train, y_train)\n",
        "rfc.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36656891495601174"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92cRKaBJBuql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################################################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewE9HTby1oSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage.feature import hog\n",
        "def feature_extraction(image):\n",
        "    return hog(color.rgb2gray(image), orientations=8, pixels_per_cell=(10, 10), cells_per_block=(5, 5))\n",
        "def predict(df):\n",
        "    predict = knn.predict(df.reshape(1,-1))[0]\n",
        "    predict_proba = knn.predict_proba(df.reshape(1,-1))\n",
        "    return predict, predict_proba[0][predict]\n",
        "digits = []\n",
        "# load your image from file\n",
        "# extract featuress\n",
        "hogs = list(map(lambda x: feature_extraction(x), digits))\n",
        "# apply k-NN model created in previous\n",
        "predictions = list(map(lambda x: predict(x), hogs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lBTv5vI1zCI",
        "colab_type": "code",
        "outputId": "7ffba1e6-3950-4518-84ef-9de34abd365d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "feature_extraction('/content/drive/My Drive/APM Project/Data/Characters only/Img/Sample029/img029-027.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-63c8702d39b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/APM Project/Data/Characters only/Img/Sample029/img029-027.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-aecfd9e2fffd>\u001b[0m in \u001b[0;36mfeature_extraction\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixels_per_cell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcells_per_block\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'color' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qut_O7W1y1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1V4_Un-ln1d",
        "colab_type": "code",
        "outputId": "57623b4d-19eb-42b4-f7c0-de4e51452a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from imutils import paths\n",
        "import argparse\n",
        "knn_model = KNeighborsClassifier(n_neighbors = 3)\n",
        "knn_model.fit(X_train, y_train)\n",
        "acc = knn_model.score(X_test, y_test)\n",
        "print('Accuracy: {:.2f}%'.format(acc*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 34.90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3TRWjk-Wolv",
        "colab_type": "text"
      },
      "source": [
        "Loading IAM Dataset images to implement model on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hykr78sMe3cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "##from SamplePreprocessor import preprocess\n",
        "\n",
        "\n",
        "class Sample:\n",
        "\t\"sample from the dataset\"\n",
        "\tdef __init__(self, gtText, filePath):\n",
        "\t\tself.gtText = gtText\n",
        "\t\tself.filePath = filePath\n",
        "\n",
        "\n",
        "class Batch:\n",
        "\t\"batch containing images and ground truth texts\"\n",
        "\tdef __init__(self, gtTexts, imgs):\n",
        "\t\tself.imgs = np.stack(imgs, axis=0)\n",
        "\t\tself.gtTexts = gtTexts\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "\t\"loads data which corresponds to IAM format, see: http://www.fki.inf.unibe.ch/databases/iam-handwriting-database\" \n",
        "\n",
        "\tdef __init__(self, filePath, batchSize, imgSize, maxTextLen):\n",
        "\t\t\"loader for dataset at given location, preprocess images and text according to parameters\"\n",
        "\n",
        "\t\tassert filePath[-1]=='/'\n",
        "\n",
        "\t\tself.dataAugmentation = False\n",
        "\t\tself.currIdx = 0\n",
        "\t\tself.batchSize = batchSize\n",
        "\t\tself.imgSize = imgSize\n",
        "\t\tself.samples = []\n",
        "\t\n",
        "\t\tf=open(filePath+'words.txt')\n",
        "\t\tchars = set()\n",
        "\t\tbad_samples = []\n",
        "\t\tbad_samples_reference = ['a01-117-05-02.png', 'r06-022-03-05.png']\n",
        "\t\tfor line in f:\n",
        "\t\t\t# ignore comment line\n",
        "\t\t\tif not line or line[0]=='#':\n",
        "\t\t\t\tcontinue\n",
        "\t\t\t\n",
        "\t\t\tlineSplit = line.strip().split(' ')\n",
        "\t\t\tassert len(lineSplit) >= 9\n",
        "\t\t\t\n",
        "\t\t\t# filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
        "\t\t\tfileNameSplit = lineSplit[0].split('-')\n",
        "\t\t\tfileName = filePath + 'words/' + fileNameSplit[0] + '/' + fileNameSplit[0] + '-' + fileNameSplit[1] + '/' + lineSplit[0] + '.png'\n",
        "\n",
        "\t\t\t# GT text are columns starting at 9\n",
        "\t\t\tgtText = self.truncateLabel(' '.join(lineSplit[8:]), maxTextLen)\n",
        "\t\t\tchars = chars.union(set(list(gtText)))\n",
        "\n",
        "\t\t\t# check if image is not empty\n",
        "\t\t\t##if not os.path.getsize(fileName):\n",
        "\t\t\t#\tbad_samples.append(lineSplit[0] + '.png')\n",
        "\t\t\t#\tcontinue\n",
        "\n",
        "\t\t\t# put sample into list\n",
        "\t\t\tself.samples.append(Sample(gtText, fileName))\n",
        "\n",
        "\t\t# some images in the IAM dataset are known to be damaged, don't show warning for them\n",
        "\t\tif set(bad_samples) != set(bad_samples_reference):\n",
        "\t\t\tprint(\"Warning, damaged images found:\", bad_samples)\n",
        "\t\t\tprint(\"Damaged images expected:\", bad_samples_reference)\n",
        "\n",
        "\t\t# split into training and validation set: 95% - 5%\n",
        "\t\tsplitIdx = int(0.95 * len(self.samples))\n",
        "\t\tself.trainSamples = self.samples[:splitIdx]\n",
        "\t\tself.validationSamples = self.samples[splitIdx:]\n",
        "\n",
        "\t\t# put words into lists\n",
        "\t\tself.trainWords = [x.gtText for x in self.trainSamples]\n",
        "\t\tself.validationWords = [x.gtText for x in self.validationSamples]\n",
        "\n",
        "\t\t# number of randomly chosen samples per epoch for training \n",
        "\t\tself.numTrainSamplesPerEpoch = 25000 \n",
        "\t\t\n",
        "\t\t# start with train set\n",
        "\t\tself.trainSet()\n",
        "\n",
        "\t\t# list of all chars in dataset\n",
        "\t\tself.charList = sorted(list(chars))\n",
        "\n",
        "\n",
        "\tdef truncateLabel(self, text, maxTextLen):\n",
        "\t\t# ctc_loss can't compute loss if it cannot find a mapping between text label and input \n",
        "\t\t# labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
        "\t\t# If a too-long label is provided, ctc_loss returns an infinite gradient\n",
        "\t\tcost = 0\n",
        "\t\tfor i in range(len(text)):\n",
        "\t\t\tif i != 0 and text[i] == text[i-1]:\n",
        "\t\t\t\tcost += 2\n",
        "\t\t\telse:\n",
        "\t\t\t\tcost += 1\n",
        "\t\t\tif cost > maxTextLen:\n",
        "\t\t\t\treturn text[:i]\n",
        "\t\treturn text\n",
        "\n",
        "\n",
        "\tdef trainSet(self):\n",
        "\t\t\"switch to randomly chosen subset of training set\"\n",
        "\t\tself.dataAugmentation = True\n",
        "\t\tself.currIdx = 0\n",
        "\t\trandom.shuffle(self.trainSamples)\n",
        "\t\tself.samples = self.trainSamples[:self.numTrainSamplesPerEpoch]\n",
        "\n",
        "\t\n",
        "\tdef validationSet(self):\n",
        "\t\t\"switch to validation set\"\n",
        "\t\tself.dataAugmentation = False\n",
        "\t\tself.currIdx = 0\n",
        "\t\tself.samples = self.validationSamples\n",
        "\n",
        "\n",
        "\tdef getIteratorInfo(self):\n",
        "\t\t\"current batch index and overall number of batches\"\n",
        "\t\treturn (self.currIdx // self.batchSize + 1, len(self.samples) // self.batchSize)\n",
        "\n",
        "\n",
        "\tdef hasNext(self):\n",
        "\t\t\"iterator\"\n",
        "\t\t#return self.currIdx + self.batchSize <= len(self.samples)\n",
        "\t\treturn self.currIdx + self.batchSize <= 10\n",
        "\t\t\n",
        "\t\t\n",
        "\tdef getNext(self):\n",
        "\t\t\"iterator\"\n",
        "\t\tbatchRange = range(self.currIdx, self.currIdx + self.batchSize)\n",
        "\t\tgtTexts = [self.samples[i].gtText for i in batchRange]\n",
        "\t\timgs = [preprocess(cv2.imread(self.samples[i].filePath), self.imgSize, self.dataAugmentation) for i in batchRange]\n",
        "\t\tself.currIdx += self.batchSize\n",
        "\t\treturn Batch(gtTexts, imgs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pupCsUX7Pld5",
        "colab_type": "code",
        "outputId": "d974c23f-5507-460f-8582-939322966ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "fnTrain = '/content/drive/My Drive/APM Project/Data/testing/'\n",
        "imgSize = (128, 32)\n",
        "maxTextLen = 32\n",
        "loader = DataLoader(fnTrain, 1, imgSize, maxTextLen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning, damaged images found: []\n",
            "Damaged images expected: ['a01-117-05-02.png', 'r06-022-03-05.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_T1eUlCVDgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# while loader.hasNext():\n",
        "#   batch = loader.getNext() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRAVLOJldodC",
        "colab_type": "code",
        "outputId": "4e5545a3-5151-4b0f-fdae-61cdfc996957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "#for testing\n",
        "for i in range(5):\n",
        "  print(loader.samples[i].filePath)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/APM Project/Data/testing/words/a03/a03-063/a03-063-06-06.png\n",
            "/content/drive/My Drive/APM Project/Data/testing/words/e07/e07-012/e07-012-02-02.png\n",
            "/content/drive/My Drive/APM Project/Data/testing/words/p03/p03-121/p03-121-06-04.png\n",
            "/content/drive/My Drive/APM Project/Data/testing/words/c03/c03-094b/c03-094b-09-01.png\n",
            "/content/drive/My Drive/APM Project/Data/testing/words/m04/m04-061/m04-061-03-12.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygo2Xqixc9lm",
        "colab_type": "code",
        "outputId": "aec843e0-35d5-4587-bac1-4b1f072c4d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "##  HARD CODING VALUES ONLY FOR TESTING PURPOSES. PLEASE REMOVE THIS WHEN THE ENTIRE DATASET IS LOADED\n",
        "\n",
        "fnTrain = '/content/drive/My Drive/APM Project/Data/testing/'\n",
        "imgSize = (128, 32)\n",
        "maxTextLen = 32\n",
        "loader = DataLoader(fnTrain, 1, imgSize, maxTextLen)\n",
        "\n",
        "loader.samples[0].filePath = '/content/drive/My Drive/APM Project/Data/testing/words/e06/e06-015/e06-015-08-03.png'\n",
        "loader.samples[1].filePath = '/content/drive/My Drive/APM Project/Data/testing/words/f01/f01-066/f01-066-00-02.png'\n",
        "loader.samples[2].filePath = '/content/drive/My Drive/APM Project/Data/testing/words/g01/g01-067/g01-067-03-04.png'\n",
        "loader.samples[3].filePath = '/content/drive/My Drive/APM Project/Data/testing/words/h01/h01-024/h01-024-03-05.png'\n",
        "loader.samples[4].filePath = '/content/drive/My Drive/APM Project/Data/testing/words/e04/e04-008/e04-008-00-09.png'\n",
        "loader.samples[5].filePath = '/content/drive/My Drive/APM Project/Data/testing/words/a05/a05-089/a05-089-04-03.png'\n",
        "loader.samples[6].filePath = '/content/drive/My Drive/APM Project/Data/testing/words/b05/b05-017/b05-017-03-05.png'\n",
        "loader.samples[7].filePath = '/content/drive/My Drive/APM Project/Data/testing/words/b06/b06-059/b06-059-05-05.png'\n",
        "loader.samples[8].filePath = '/content/drive/My Drive/APM Project/Data/testing/words/c03/c03-021d/c03-021d-00-01.png'\n",
        "loader.samples[9].filePath = '/content/drive/My Drive/APM Project/Data/testing/words/d01/d01-056/d01-056-07-01.png'\n",
        "\n",
        "loader.samples[0].gtText = 'before'\n",
        "loader.samples[1].gtText = 'you'\n",
        "loader.samples[2].gtText = 'read'\n",
        "loader.samples[3].gtText = 'business'\n",
        "loader.samples[4].gtText = 'pinned'\n",
        "loader.samples[5].gtText = 'I'\n",
        "loader.samples[6].gtText = 'for'\n",
        "loader.samples[7].gtText = 'u'\n",
        "loader.samples[8].gtText = 'Fanny'\n",
        "loader.samples[9].gtText = 'been'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning, damaged images found: []\n",
            "Damaged images expected: ['a01-117-05-02.png', 'r06-022-03-05.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVmXiJNfeRwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = []\n",
        "y = []\n",
        "while loader.hasNext():\n",
        "  batch = loader.getNext() \n",
        "  y.extend(batch.gtTexts)\n",
        "  x.extend(batch.imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP0YCOVqWkYo",
        "colab_type": "text"
      },
      "source": [
        "Now creating the sliding window"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi4fzrneUya4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Finding the countours\n",
        "cells = []\n",
        "letter_pointer_df = pd.DataFrame(columns = ['Image', 'intX','intY','intW','intH']) # Dataframe to store green bounding box pixel positions\n",
        "\n",
        "for i in range(len(x)):\n",
        "  imgInput = x[i]\n",
        "  # cv2_imshow(imgInput)\n",
        "  if imgInput is None:\n",
        "    print ('test')\n",
        "  # imgGray = cv2.cvtColor(imgInput, cv2.COLOR_BGR2GRAY) # Convert to grayscale since it's easier to detect bright objects against dark background\n",
        "  newRet, binaryThreshold = cv2.threshold(imgInput,127,255,cv2.THRESH_BINARY_INV) #inverting black and white regions, making the letters white\n",
        "  rectkernel = cv2.getStructuringElement(cv2.MORPH_RECT,(15,10)) # Build a kernel to scan over the image\n",
        "  rectdilation = cv2.dilate(binaryThreshold, rectkernel, iterations = 1) #Dilate the image values to create good enough contrast\n",
        "  outputImage = imgInput.copy() \n",
        "  # cv2_imshow(outputImage)\n",
        "  rand_var, npaContours, npaHierarchy = cv2.findContours(rectdilation.copy(),        \n",
        "                                              cv2.RETR_EXTERNAL,                 \n",
        "                                              cv2.CHAIN_APPROX_SIMPLE)  #Creating contours to find position of the word in the image\n",
        "\n",
        "  for npaContour in npaContours:                  \n",
        "      if cv2.contourArea(npaContour) > 120:       #Change this value   #Filtering on minimum contour area (need to optimize this)\n",
        "\n",
        "          [intX, intY, intW, intH] = cv2.boundingRect(npaContour)        #Find Red bounding box (outer box for word)\n",
        "\n",
        "          cv2.rectangle(outputImage,          #For display purposes, drawing red rectangle on the image \n",
        "                (intX, intY),                 # upper left corner\n",
        "                (intX+intW,intY+intH),        # lower right corner\n",
        "                (0, 0, 255),                  # red\n",
        "                2)                            # thickness\n",
        "\n",
        "          # Get subimage of word and find contours of that word\n",
        "          imgROI = binaryThreshold[intY:intY+intH, intX:intX+intW]   \n",
        "\n",
        "\n",
        "          rand_var2, subContours, subHierarchy = cv2.findContours(imgROI.copy(),     #Finding countours for individual characters   \n",
        "                                              cv2.RETR_EXTERNAL,                 \n",
        "                                              cv2.CHAIN_APPROX_SIMPLE) \n",
        "          letter_pointer_df = letter_pointer_df.append({'Image': imgInput.copy(), 'intX':intX, 'intY': intY, 'intW' : intW, 'intH' : intH}, ignore_index=True) #Storing all bounding boxes \n",
        "  #cells.append(gray)\n",
        "  #cv2_imshow(outputImage)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNk1TObTfrja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input mapping file\n",
        "mapping_file = pd.read_csv('/content/drive/My Drive/APM Project/Data/Characters only/Labels to actual character file.csv',header = None)\n",
        "mapping_file.columns = ['Mapping','Label']\n",
        "\n",
        "map_dict = mapping_file['Label'].to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROIT_4KukrCK",
        "colab_type": "code",
        "outputId": "c619996a-0463-465b-f15d-915775d0f899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "map_dict[20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'K'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEv_MUqkXLDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sliding_window_size = (43,32)\n",
        "#Retreiving characters from formatted word image using bounding box\n",
        "predicted_labels = []\n",
        "for i in range(len(letter_pointer_df)):\n",
        "  Image,intX,intY,intW,intH = letter_pointer_df.iloc[i]\n",
        "  imgCopy =Image.copy()\n",
        "  original_intX = intX\n",
        "  sliding_window_img_list = []\n",
        "  label_string = ''\n",
        "  while (intX+sliding_window_size[0] <= original_intX + intW):\n",
        "    sliding_window_img = imgCopy[intY:intY+sliding_window_size[1], intX:intX+sliding_window_size[0]]\n",
        "    sliding_window_img_list.append(sliding_window_img)\n",
        "    intX = intX + 20\n",
        "  sliding_window_img_list = (np.asarray(sliding_window_img_list)).reshape(-1,1376).astype(np.float32) \n",
        "  # sliding_window_img_list = np.asarray(sliding_window_img_list,dtype = 'int64')[:,np.newaxis]\n",
        "  ret, result, neighbours, dist = knn.findNearest(sliding_window_img_list, k=3)\n",
        "  for j in range(len(result)):\n",
        "    label_string = label_string + map_dict[int(result[j])] \n",
        "  predicted_labels.append(label_string)\n",
        " # cv2_imshow(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwHmBVzvk7-S",
        "colab_type": "code",
        "outputId": "786eb1e8-435d-4139-d610-6400e8da44f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "predicted_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aaiic',\n",
              " '8BTTT',\n",
              " 'ieIig',\n",
              " 'wrjua',\n",
              " 'a4d0c',\n",
              " 'yNOH4',\n",
              " 'eH4Fw',\n",
              " 'cdCLe',\n",
              " 'cJT41',\n",
              " 'j1a1I']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyfM8KtOaoso",
        "colab_type": "code",
        "outputId": "51462956-6329-4258-adaa-ef21a50f4037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Now we check the accuracy of classification\n",
        "# For that, compare the result with test_labels and check which are wrong\n",
        "matches = result==y_test\n",
        "correct = np.count_nonzero(matches)\n",
        "accuracy = correct*100.0/result.size\n",
        "print (accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AV3xTSAarx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}